{
  "q": "Flash Attention mechanism"
}
{
  "results": [
    {
      "full_content": "Error fetching content",
      "link": "https://www.marktechpost.com/2022/03/07/google-and-cornell-researchers-introduce-flash-a-machine-learning-model-that-can-achieve-high-transformer-quality-in-linear-time/",
      "summary": "Error fetching summary",
      "title": "Google and Cornell Researchers Introduce FLASH: A Machine ..."
    },
    {
      "full_content": "Assistance in building custom Encoding/Attention mechanism ¬∑ Issue #558 ¬∑ facebookresearch/xformers ¬∑ GitHub Skip to content Toggle navigation Sign up Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions For Enterprise Teams Startups Education By Solution CI/CD & Automation DevOps DevSecOps Case Studies Customer Stories Resources Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Pricing In this repository All GitHub ‚Üµ Jump to ‚Üµ No suggested jump to results In this repository All GitHub ‚Üµ Jump to ‚Üµ In this organization All GitHub ‚Üµ Jump to ‚Üµ In this repository All GitHub ‚Üµ Jump to ‚Üµ Sign in Sign up facebookresearch / xformers Public Notifications Fork 286 Star 4k Code Issues 92 Pull requests 7 Discussions Actions Projects 0 Security Insights More Code Issues Pull requests Discussions Actions Projects Security Insights New issue Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community. Pick a username Email Address Password Sign up for GitHub By clicking ‚ÄúSign up for GitHub‚Äù, you agree to our terms of service and privacy statement . We‚Äôll occasionally send you account related emails. Already on GitHub? Sign in to your account Jump to bottom Assistance in building custom Encoding/Attention mechanism #558 Closed SeanNaren opened this issue Dec 7, 2022 ¬∑ 26 comments Closed Assistance in building custom Encoding/Attention mechanism #558 SeanNaren opened this issue Dec 7, 2022 ¬∑ 26 comments Comments Copy link Contributor SeanNaren commented Dec 7, 2022 üöÄ Feature We are hoping to incorporate xFormers into https://github.com/NVIDIA/NeMo , particularly for our ASR Conformer models. There seems to be two blocking modules for this; Relative Positional Embeddings ( here ) and our modifications to the Base MHA to include the relative shift ( here ) Does anyone on the team know how this can be implemented/worked-around? Happy to help just need some guidance if so! cc @blefaudeux @titu1994 The text was updated successfully, but these errors were encountered: All reactions Copy link Contributor danthe3rd commented Dec 7, 2022 ‚Ä¢ edited Hi @SeanNaren and thanks for opening this issue. First, we would be excited to have xformers used in NeMo! We don't have plans to include relative positional embeddings or relative shift at the moment, but we have some other customers who were also interested, so we would be happy to give pointers and accept contributions :) What device/data type are you interested in for training? I would also advise you to wait until we are done with the refactor of the MHA part before implementing new things ‚ù§Ô∏è 3 SeanNaren, titu1994, and grazder reacted with heart emoji üöÄ 1 SeanNaren reacted with rocket emoji All reactions ‚ù§Ô∏è 3 reactions üöÄ 1 reaction Sorry, something went wrong. Copy link Contributor Author SeanNaren commented Dec 7, 2022 Thanks @danthe3rd ! That's awesome :) happy to look into this! Currently our priorities are GPU (v100/ampere) for float32,BF16 Let me know what I should look into code wise for this, might not have time now but sure I will in the near future All reactions Sorry, something went wrong. Copy link Contributor danthe3rd commented Dec 7, 2022 xformers currently has multiple kernels, and dispatching happens depending on the hardware/sequence length/batch size and more. If you want to run on f32 OR V100, the kernel based on CUTLASS will be the only one supported (CUDA source code for forward: https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/csrc/cuda/mem_eff_attention/kernel_forward.h ). If you want to run on bf16/A100, the CUTLASS-based kernel can still run, although triton/C++ kernels written by @tridao will be usually faster In terms of ease of use, it might be easier to modify the triton kernel, but it's also prone to bugs as sometimes triton just won't work properly. Are you interested in training or inference only? Do you need to back-propagate through the bias for your rel positional embeddings? In terms of workaround, you can also consider other biases - like ROPE which does not require to modify the attention matrix itself. All reactions Sorry, something went wrong. Copy link titu1994 commented Dec 7, 2022 ‚Ä¢ edited Training would be good, but inference alone is also fine for a start. I've tried RoPE previously and the issue is that for ASR, the audio range is significantly different between training and inference (train is roughly 20 seconds ~ 500 timesteps in the encoder, eval is upwards of 2-3 mins for encoder ~ 2250+ timesteps, and with some modifications we can go to near 30 mins of speech ~ 22500 timesteps) Till now, only relative positional encodings have helped during inference of such audio duration. Unless you're referring to something else by RoPE (there's i think 3 types with the same name) üëç 1 SeanNaren reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Copy link Contributor danthe3rd commented Dec 7, 2022 If we are only targeting inference, you should be able to get started with the triton kernel. Just recently merged, it supports masks - should run on Sm75+ - maybe Sm70 but needs some more testing. Should be straightforward to use after the ongoing refactor :) üëç 1 SeanNaren reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Copy link Contributor Author SeanNaren commented Jan 13, 2023 @danthe3rd did the refactor get merged? An...
}
